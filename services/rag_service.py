from transformers import AutoModelForCausalLM, AutoTokenizer
from data_access.chromadb_repo import get_similar_documents
import torch
import gc

def rag_inference(user_input: str) -> str:
    """
    Perform RAG (Reinforcement-Aided Generation) inference using a pre-trained model.

    Args:
        user_input: The user's input.

    Returns:
        A response generated by the RAG model.
    """
    similar_documents = get_similar_documents(user_input, n=1)

    model_id = "mistralai/Mistral-7B-Instruct-v0.3"
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    conversation = [
        {"role": "system",
         "content": "You are a helpful AI assistant that provides groudned answers. "
                    "Answer only based on the provided context."
         },
        {"role": "user", "content": f'###CONTEXT{similar_documents}###QUESTION:{user_input}!@#$%'}
    ]

    inputs = tokenizer.apply_chat_template(
        conversation,
        add_generation_prompt=True,
        return_dict=True,
        return_tensors="pt",
    )

    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="auto")
    inputs.to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=1000)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    del model
    gc.collect()
    torch.cuda.empty_cache()
    answer = answer.split("!@#$%")[-1].strip()

    return answer


if __name__ == '__main__':
    print(rag_inference("What is the email of mr. Goulianas?"))